---
title: "Sampling Distribution of Regression Parameter Estimates"
output: 
  html_document:
    theme: flatly
    code_folding: hide
---

```{r}
library(pander)
library(sf)
```



## {.tabset .tabset-pills }

### Sampling Distributions

#### Sampling Distributions

Definition - A Sampling Distribution is a probability distribution of a given random sample based statistic. They describe the values that they could take on over different samples. In the graph below, a simulation showing sampling distributions, the grey area represents all of the different samples that one could get from an infinite amount of samples, also known as the population.The green line represent the amount of standard deviations they are away from the mean. A normal, Gaussian, distribution is represented by a bell curve where 1 standard deviation represents 68.26% of the observed values, 2 standard deviations represents 95.44% of the observed values and 3 standard deviations represent 99.72% of observed values. 



```{r, message=FALSE, warning=FALSE}
## Simulation to Show relationship between Standard Errors

##-----------------------------------------------
## Edit anything in this area... 

n <- 100 #sample size
Xstart <- 30 #lower-bound for x-axis
Xstop <- 100 #upper-bound for x-axis

beta_0 <- 2 #choice of true y-intercept
beta_1 <- 3.5 #choice of true slope
sigma <- 13.8 #choice of st. deviation of error terms

## End of Editable area.
##-----------------------------------------------
```

**True Model**

$$
  Y_i = \overbrace{\beta_0}^{`r beta_0`} + \overbrace{\beta_1}^{`r beta_1`} X_i + \epsilon_i \quad \text{where} \ \epsilon_i \sim N(0, \overbrace{\sigma^2}^{\sigma=`r sigma`})
$$

```{r, fig.height=8, fig.width=8, message=FALSE, warning=FALSE}
X <- rep(seq(Xstart,Xstop, length.out=n/2), each=2) #Create X
N <- 5000 #number of times to pull a random sample
storage_b0 <- storage_b1 <- storage_rmse <- rep(NA, N)
for (i in 1:N){
  Y <- beta_0 + beta_1*X + rnorm(n, 0, sigma) #Sample Y from true model
  mylm <- lm(Y ~ X)
  storage_b0[i] <- coef(mylm)[1]
  storage_b1[i] <- coef(mylm)[2]
  storage_rmse[i] <- summary(mylm)$sigma
}


layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE), widths=c(2,2), heights=c(3,3))

Ystart <- 0 #min(0,min(Y)) 
Ystop <- 500 #max(max(Y), 0)
Yrange <- Ystop - Ystart

plot(Y ~ X, xlim=c(min(0,Xstart-2), max(0,Xstop+2)), 
     ylim=c(Ystart, Ystop), pch=16, col="gray",
     main="Regression Lines from many Samples\n Plus Residual Standard Deviation Lines")
text(Xstart, Ystop, bquote(sigma == .(sigma)), pos=1)
text(Xstart, Ystop-.1*Yrange, bquote(sum ((x[i]-bar(x))^2, i==1, n) == .(var(X)*(n-1))), pos=1)
text(Xstart, Ystop-.25*Yrange, bquote(sqrt(MSE) == .(mean(storage_rmse))), pos=1)


for (i in 1:N){
  abline(storage_b0[i], storage_b1[i], col="darkgray")  
}
abline(beta_0, beta_1, col="green", lwd=3)
abline(beta_0+sigma, beta_1, col="green", lwd=2)
abline(beta_0-sigma, beta_1, col="green", lwd=2)
abline(beta_0+2*sigma, beta_1, col="green", lwd=1)
abline(beta_0-2*sigma, beta_1, col="green", lwd=1)
abline(beta_0+3*sigma, beta_1, col="green", lwd=.5)
abline(beta_0-3*sigma, beta_1, col="green", lwd=.5)

par(mai=c(1,.6,.5,.01))

  addnorm <- function(m,s, col="firebrick"){
    curve(dnorm(x, m, s), add=TRUE, col=col, lwd=2)
    lines(c(m,m), c(0, dnorm(m,m,s)), lwd=2, col=col)
    lines(rep(m-s,2), c(0, dnorm(m-s, m, s)), lwd=2, col=col)
    lines(rep(m-2*s,2), c(0, dnorm(m-2*s, m, s)), lwd=2, col=col)
    lines(rep(m-3*s,2), c(0, dnorm(m-3*s, m, s)), lwd=2, col=col)
    lines(rep(m+s,2), c(0, dnorm(m+s, m, s)), lwd=2, col=col)
    lines(rep(m+2*s,2), c(0, dnorm(m+2*s, m, s)), lwd=2, col=col)
    lines(rep(m+3*s,2), c(0, dnorm(m+3*s, m, s)), lwd=2, col=col)
    legend("topleft", legend=paste("Std. Error = ", round(s,3)), cex=0.7, bty="n")
  }

  h0 <- hist(storage_b0, 
             col="skyblue3", 
             main="Sampling Distribution\n Y-intercept",
             xlab=expression(paste("Estimates of ", beta[0], " from each Sample")),
             freq=FALSE, yaxt='n', ylab="")
  m0 <- mean(storage_b0)
  s0 <- sd(storage_b0)
  addnorm(m0,s0, col="green")
  
  h1 <- hist(storage_b1, 
             col="skyblue3", 
             main="Sampling Distribution\n Slope",
             xlab=expression(paste("Estimates of ", beta[1], " from each Sample")),
             freq=FALSE, yaxt='n', ylab="")
  m1 <- mean(storage_b1)
  s1 <- sd(storage_b1)
  addnorm(m1,s1, col="green")



```









#### Standard Error Measurement 

In statistics, the goal is to gather conclusions representing the entire population, not the particular sample that was observed. Standard Error is another way that can measure that. In a simple regression $\beta_0$ and $\beta_1$ are representative of a population The standard error indicates how different the population mean will be from the sample mean.  

```{r, message=FALSE, warning=FALSE}


lm.cars <- lm(dist ~ speed, data=cars)
sum_lmcars <- summary(lm.cars) 
pander(sum_lmcars)

```


### P-Values

#### P-Values

 Definition - The P-value represents the probability of obtaining results at least as extreme as the ones observed results of the statistical hypothesis. The p-values serves as an indication to whether or not the null hypothesis needs to be rejected. The smaller the p-value the greater the significance there is in the observed results of the analysis. The logic behind the p-value is a measure of how unlikely that it is that the observed values are random, and that they are most likely repeatable under a different observation. In short, the p-value represents the likely hood of an observation of values being repeated. In the graph below, the grey shaded area represents all of the possibilities that could be observed for a given population. They p-value represents the likelihood of a data set have a relationship as such.
 


```{r, fig.height=8, fig.width=8, message=FALSE, warning=FALSE}

X <- rep(seq(Xstart,Xstop, length.out=n/2), each=2) #Create X
N <- 5000 #number of times to pull a random sample
storage_b0 <- storage_b1 <- storage_rmse <- rep(NA, N)
for (i in 1:N){
  Y <- beta_0 + beta_1*X + rnorm(n, 0, sigma) #Sample Y from true model
  mylm <- lm(Y ~ X)
  storage_b0[i] <- coef(mylm)[1]
  storage_b1[i] <- coef(mylm)[2]
  storage_rmse[i] <- summary(mylm)$sigma
}


layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE), widths=c(2,2), heights=c(3,3))

Ystart <- 0 #min(0,min(Y)) 
Ystop <- 500 #max(max(Y), 0)
Yrange <- Ystop - Ystart

plot(Y ~ X, xlim=c(min(0,Xstart-2), max(0,Xstop+2)), 
     ylim=c(Ystart, Ystop), pch=16, col="gray",
     main="Regression Lines from many Samples\n Plus Residual Standard Deviation Lines")
text(Xstart, Ystop, bquote(sigma == .(sigma)), pos=1)
text(Xstart, Ystop-.1*Yrange, bquote(sum ((x[i]-bar(x))^2, i==1, n) == .(var(X)*(n-1))), pos=1)
text(Xstart, Ystop-.25*Yrange, bquote(sqrt(MSE) == .(mean(storage_rmse))), pos=1)


for (i in 1:N){
  abline(storage_b0[i], storage_b1[i], col="darkgray")  
}
# abline(beta_0, beta_1, col="green", lwd=3)
# abline(beta_0+sigma, beta_1, col="green", lwd=2)
# abline(beta_0-sigma, beta_1, col="green", lwd=2)
# abline(beta_0+2*sigma, beta_1, col="green", lwd=1)
# abline(beta_0-2*sigma, beta_1, col="green", lwd=1)
# abline(beta_0+3*sigma, beta_1, col="green", lwd=.5)
# abline(beta_0-3*sigma, beta_1, col="green", lwd=.5)

par(mai=c(1,.6,.5,.01))

  addnorm <- function(m,s, col="firebrick"){
    curve(dnorm(x, m, s), add=TRUE, col=col, lwd=2)
    lines(c(m,m), c(0, dnorm(m,m,s)), lwd=2, col=col)
    lines(rep(m-s,2), c(0, dnorm(m-s, m, s)), lwd=2, col=col)
    lines(rep(m-2*s,2), c(0, dnorm(m-2*s, m, s)), lwd=2, col=col)
    lines(rep(m-3*s,2), c(0, dnorm(m-3*s, m, s)), lwd=2, col=col)
    lines(rep(m+s,2), c(0, dnorm(m+s, m, s)), lwd=2, col=col)
    lines(rep(m+2*s,2), c(0, dnorm(m+2*s, m, s)), lwd=2, col=col)
    lines(rep(m+3*s,2), c(0, dnorm(m+3*s, m, s)), lwd=2, col=col)
    legend("topleft", legend=paste("Std. Error = ", round(s,3)), cex=0.7, bty="n")
  }


```



### Confidence Intervals

#### Confidence Intervals

Definition - A confidence error is the mean of $\hat{Y}_i$ plus of minus the variation of $\hat{Y}_i$. In statistics, this represents the level of confidence that you have in your observed values occurring again under a different observation. In other words, if the confidence interval for a model is at a 95% confidence level, it is stating that $\hat{Y}_i$ will be between the lower and upper levels of the confidence interval 95 out of 100 times.The table and chart below represents a regression from the cars library of stopping distance and speed. The dashed lines represent the confidence interval of the regression. Just from looking at the chart one can notice that roughly 95% of the observed values are located within the confidence interval in this observation. 


```{r, message=FALSE, warning=FALSE}

# 
# lm.cars <- lm(dist ~ speed, data=cars)
# sum_lmcars <- summary(lm.cars) 
# pander(sum_lmcars)

```

```{r, message=FALSE, warning=FALSE}

plot(dist ~ speed, data=cars)
abline(lm.cars)
abline(-17.5791 + 15.38, 3.9324, lty=2)
abline(-17.5791 - 15.38, 3.9324, lty=2)

```

###







